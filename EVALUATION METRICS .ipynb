{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":23812,"sourceType":"datasetVersion","datasetId":17810},{"sourceId":11553250,"sourceType":"datasetVersion","datasetId":7244692},{"sourceId":356050,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":296912,"modelId":317527},{"sourceId":356056,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":296917,"modelId":317532}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import load_model\nimport os\nimport keras\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom collections import deque\nimport copy\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T10:31:54.380307Z","iopub.execute_input":"2025-04-25T10:31:54.380861Z","iopub.status.idle":"2025-04-25T10:31:54.384902Z","shell.execute_reply.started":"2025-04-25T10:31:54.380840Z","shell.execute_reply":"2025-04-25T10:31:54.384149Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"model = load_model(\"/kaggle/input/detection-model/tensorflow2/default/1/best_model_high_res_512_grayscale (1).h5\")\nlast_conv_layer = \"last_conv_layer\"\nsegmentation_model = load_model(\"/kaggle/input/segmentation_model/tensorflow2/default/1/trained_model.hdf5\", compile=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T10:32:11.705480Z","iopub.execute_input":"2025-04-25T10:32:11.706139Z","iopub.status.idle":"2025-04-25T10:32:12.624710Z","shell.execute_reply.started":"2025-04-25T10:32:11.706112Z","shell.execute_reply":"2025-04-25T10:32:12.624152Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def get_arr_img_segmentation(img_path):\n    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    img = cv2.resize(img, (256, 256))\n    img = img.astype(\"float32\") / 255.0\n    img = np.expand_dims(img, axis=-1) \n    img = np.expand_dims(img, axis=0)\n    return img\n    \ndef get_arr_img(img_path):\n    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    img = cv2.resize(img, (512, 512))\n    img = img.astype(\"float32\") / 255.0\n    img = np.expand_dims(img, axis=-1) \n    img = np.expand_dims(img, axis=0)\n    return img\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    grad_model = keras.models.Model(\n        model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    \n    heatmap = tf.squeeze(heatmap)\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    heatmap = tf.image.resize(\n        heatmap[..., tf.newaxis],  # Add channel dim (now shape: [H, W, 1])\n        (512, 512),\n        method=tf.image.ResizeMethod.BILINEAR  # Smooth interpolation\n    )\n    return heatmap.numpy()\n\ndef make_gradcam_plus_plus_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    grad_model = keras.models.Model(\n        [model.inputs],\n        [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n    with tf.GradientTape() as tape1:\n        with tf.GradientTape() as tape2:\n            with tf.GradientTape() as tape3:\n                conv_output, predictions = grad_model(img_array)\n                if pred_index is None:\n                    pred_index = tf.argmax(predictions[0])\n                target_class_score = predictions[:, pred_index]\n            grads = tape3.gradient(target_class_score, conv_output)\n        grads2 = tape2.gradient(grads, conv_output)\n    grads3 = tape1.gradient(grads2, conv_output)\n\n    # Compute weights\n    first_term = tf.square(grads)\n    second_term = grads2\n    third_term = conv_output\n\n    # Avoid division by zero\n    denominator = 2 * grads2 + grads3 * third_term\n    denominator = tf.where(denominator != 0.0, denominator, tf.ones_like(denominator))\n\n    alphas = first_term / denominator\n    alpha_normalization_constant = tf.reduce_sum(alphas, axis=(1, 2), keepdims=True)\n    alphas /= alpha_normalization_constant\n\n    # ReLU of gradients\n    relu_grads = tf.nn.relu(grads)\n\n    # Weight the channels by alpha * relu gradient\n    weights = tf.reduce_sum(alphas * relu_grads, axis=(1, 2))\n\n    # Weighted combination of forward activations\n    cam = tf.reduce_sum(weights[:, tf.newaxis, tf.newaxis, :] * conv_output, axis=-1)\n    heatmap = tf.squeeze(cam)\n\n    # Normalize\n    heatmap = tf.maximum(heatmap, 0)\n    max_val = tf.reduce_max(heatmap)\n    if max_val != 0:\n        heatmap /= max_val\n    heatmap = tf.image.resize(\n        heatmap[..., tf.newaxis],  # shape [H, W, 1]\n        (512, 512),  # adjust size as needed\n        method=tf.image.ResizeMethod.BILINEAR\n    )\n\n    return heatmap.numpy()\n\ndef compute_occlusion_sensitivity(\n    img_array, model, patch_size=64, stride=16, baseline_value=0.5):\n    \n    input_img = img_array.copy()\n    H, W = input_img.shape[1], input_img.shape[2]\n    heatmap = np.zeros((H, W))\n\n    # Get the original prediction\n    original_pred = model.predict(img_array, verbose=0)[0][0]\n\n    for y in range(0, H, stride):\n        for x in range(0, W, stride):\n            # Create a copy and occlude the patch\n            occluded_img = input_img.copy()\n            y1, y2 = y, min(y + patch_size, H)\n            x1, x2 = x, min(x + patch_size, W)\n            occluded_img[0, y1:y2, x1:x2, :] = baseline_value\n\n            # Get new prediction\n            pred = model.predict(occluded_img, verbose=0)[0][0]\n\n            # The drop in prediction score is used as importance\n            score_drop = original_pred - pred\n            heatmap[y1:y2, x1:x2] += score_drop\n\n    # Normalize heatmap\n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= np.max(heatmap) + 1e-8\n\n    # Resize to match original image\n    heatmap = tf.image.resize(heatmap[..., np.newaxis], (512, 512)).numpy().squeeze()\n    return heatmap\n\ndef scorecam_heatmap(img_array, model, last_conv_layer_name, class_index=None):\n    \n    conv_layer_model = keras.Model(model.inputs, model.get_layer(last_conv_layer_name).output)\n    conv_output = conv_layer_model(img_array)  # Shape: (1, H, W, N)\n    conv_output = conv_output[0]  # Shape: (H, W, N)\n\n    pred = model(img_array)\n    if class_index is None:\n        class_index = tf.argmax(pred[0])\n\n    heatmap = np.zeros(shape=conv_output.shape[:2], dtype=np.float32)  \n    for i in range(conv_output.shape[-1]):\n        activation = conv_output[..., i]\n\n        # Normalize activation to [0, 1]\n        activation -= tf.reduce_min(activation)\n        activation /= tf.reduce_max(activation) + 1e-10\n        upsampled = tf.image.resize(activation[..., tf.newaxis], (img_array.shape[1], img_array.shape[2]))\n        upsampled_img = upsampled.numpy()\n\n        # Multiply original image by upsampled activation map (masking)\n        masked_input = img_array * upsampled_img\n\n        # Get score for the target class\n        score = model(masked_input)[0, class_index].numpy()\n        heatmap += score * activation.numpy()\n\n    # Normalize heatmap\n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= np.max(heatmap) + 1e-10\n\n    # Resize to input image size (if needed)\n    heatmap = tf.image.resize(heatmap[..., np.newaxis], (img_array.shape[1], img_array.shape[2]))\n    return heatmap.numpy().squeeze()\n\ndef make_smoothgrad_heatmap(img_array, model, class_index=None, n_samples=50, noise_level=0.2):\n    if class_index is None:\n        pred = model(img_array)\n        class_index = tf.argmax(pred[0])\n\n    grads_list = []\n\n    for _ in range(n_samples):\n        noise = tf.random.normal(shape=img_array.shape, stddev=noise_level)\n        noisy_input = img_array + noise\n        with tf.GradientTape() as tape:\n            tape.watch(noisy_input)\n            preds = model(noisy_input)\n            loss = preds[:, class_index]\n        grads = tape.gradient(loss, noisy_input)  # Shape: (1, H, W, 1)\n        grads_list.append(grads[0])  # Remove batch dim\n\n    # Average the gradients\n    avg_grads = tf.reduce_mean(tf.stack(grads_list), axis=0)  # Shape: (H, W, 1)\n    avg_grads = tf.abs(avg_grads)  # Optional: Take absolute value for visualization\n\n    # Normalize to [0, 1]\n    heatmap = avg_grads - tf.reduce_min(avg_grads)\n    heatmap /= tf.reduce_max(heatmap) + 1e-10\n\n    # Resize to match original image shape\n    heatmap = tf.image.resize(heatmap, (img_array.shape[1], img_array.shape[2]))\n\n    return heatmap.numpy().squeeze()\n\ndef make_integrated_gradients_heatmap(\n    img_array,\n    model,\n    class_index=None,\n    baseline=None,\n    steps=50\n):\n        # Convert input to float32 if it's not already\n    img_array = tf.cast(img_array, tf.float32)\n    \n    # If baseline is not provided, use a black image\n    if baseline is None:\n        baseline = tf.zeros_like(img_array)\n    else:\n        baseline = tf.cast(baseline, tf.float32)\n    \n    # Interpolate inputs between baseline and actual image\n    alphas = tf.linspace(0.0, 1.0, steps + 1)\n    interpolated = [baseline + alpha * (img_array - baseline) for alpha in alphas]\n    interpolated = tf.concat(interpolated, axis=0)  # Shape: (steps+1, H, W, 1)\n    \n    # Compute gradients\n    with tf.GradientTape() as tape:\n        tape.watch(interpolated)\n        preds = model(interpolated)\n        class_logits = preds[:, class_index]\n    \n    grads = tape.gradient(class_logits, interpolated)\n    \n    # Approximate the integral using the trapezoidal rule\n    grads = (grads[:-1] + grads[1:]) / 2.0\n    avg_grads = tf.reduce_mean(grads, axis=0)\n    \n    # Calculate integrated gradients\n    integrated_grads = (img_array - baseline) * avg_grads\n    \n    # Normalize to [0, 1]\n    heatmap = avg_grads - tf.reduce_min(avg_grads)\n    heatmap /= tf.reduce_max(heatmap) + 1e-10\n\n    # Resize to match original image shape\n    heatmap = tf.image.resize(heatmap, (img_array.shape[1], img_array.shape[2]))\n\n    return heatmap.numpy().squeeze()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T10:32:13.586829Z","iopub.execute_input":"2025-04-25T10:32:13.587139Z","iopub.status.idle":"2025-04-25T10:32:13.826735Z","shell.execute_reply.started":"2025-04-25T10:32:13.587116Z","shell.execute_reply":"2025-04-25T10:32:13.825924Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# 300 random images\nimages_arr = []\nwith open(\"/kaggle/input/random-paths/random_paths.txt\", \"r\") as file:\n    for line in file:\n        path = line.strip()\n        arr = get_arr_img(path)\n        images_arr.append(arr)\n\nprint(f\"{len(images_arr)} images\" )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T23:34:32.944717Z","iopub.execute_input":"2025-04-24T23:34:32.945373Z","iopub.status.idle":"2025-04-24T23:34:40.361550Z","shell.execute_reply.started":"2025-04-24T23:34:32.945348Z","shell.execute_reply":"2025-04-24T23:34:40.360790Z"}},"outputs":[{"name":"stdout","text":"300 images\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"headers = [\"Robustness\", \"Faithfulness\", \"Complexity\", \"Randomization\"]\nmethods = [\n    \"Grad-CAM\", \n    \"Grad-CAM++\", \n    \"Score-CAM\", \n    \"Integrated Gradients\", \n    \"SmoothGrad\", \n    \"Occlusion Sensitivity\",\n    \"GradCAM + Lung segment\"\n]\ndf = pd.DataFrame(index=methods, columns=headers)\ndf.fillna(\"-\", inplace=True)  \ndisplay(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T23:34:45.504372Z","iopub.execute_input":"2025-04-24T23:34:45.505008Z","iopub.status.idle":"2025-04-24T23:34:45.530860Z","shell.execute_reply.started":"2025-04-24T23:34:45.504979Z","shell.execute_reply":"2025-04-24T23:34:45.530215Z"},"jupyter":{"source_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                       Robustness Faithfulness Complexity Randomization\nGrad-CAM                        -            -          -             -\nGrad-CAM++                      -            -          -             -\nScore-CAM                       -            -          -             -\nIntegrated Gradients            -            -          -             -\nSmoothGrad                      -            -          -             -\nOcclusion Sensitivity           -            -          -             -\nGradCAM + Lung segment          -            -          -             -","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Robustness</th>\n      <th>Faithfulness</th>\n      <th>Complexity</th>\n      <th>Randomization</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Grad-CAM</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>Grad-CAM++</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>Score-CAM</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>Integrated Gradients</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>SmoothGrad</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>Occlusion Sensitivity</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>GradCAM + Lung segment</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"<center><h1>Grad-CAM</h1></center>","metadata":{}},{"cell_type":"code","source":"#________________________________________________________________________________________________________________________GRAD-CAM\n#____________________________________________________________________________________________ROBUSTNESS\nab = []\nab = images_arr\ndef explanation_robustness(original_explanation, image):\n    epsilon=0.01\n    noise = np.random.normal(loc=0.0, scale=epsilon, size=image.shape)\n    perturbed_img = np.clip(image + noise, 0, 1)\n    expl = make_gradcam_heatmap(perturbed_img, model, last_conv_layer)\n    diffs = np.linalg.norm(original_explanation - expl)\n    return round(np.mean(diffs), 3)\nscores = []\n    \nfor arr in ab:\n    heatmap = make_gradcam_heatmap(arr, model, last_conv_layer)\n    score = explanation_robustness(heatmap, arr)\n    scores.append(score)    \n    \n        \ndf.at[\"Grad-CAM\", \"Robustness\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________FAITHFULNESS\ndef faithfulness(model, image):        \n    k=0.20\n    p0 = round(model.predict(image, verbose=0)[0][0], 3)\n    heatmap = make_gradcam_heatmap(image, model, last_conv_layer)\n    threshold = np.percentile(heatmap, 100 - k*100)\n    mask = (heatmap >= threshold).astype(bool)\n        \n    image_deleted = np.copy(tf.squeeze(image).numpy())\n    image_deleted = np.expand_dims(image_deleted, axis=-1) \n    image_deleted[mask] = 0 \n    \n    image_deleted = np.expand_dims(image_deleted, 0)\n    p1 = round(model.predict(image_deleted, verbose=0)[0][0], 3)\n    return abs(p0-p1)\n\nscores = []\n\nfor arr in ab:\n    score = faithfulness(model, arr)\n    scores.append(score)\ndf.at[\"Grad-CAM\", \"Faithfulness\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________COMPLEXITY\ndef explanation_complexity(explanation, threshold=0.1):\n    \"\"\"\n    Measures the sparsity (number of important pixels).\n    \"\"\"\n    total_pixels = explanation.size\n    important_pixels = np.sum(explanation > threshold)\n    return important_pixels / total_pixels  # lower = more interpretable\n\n\nscores = []\n\nfor arr in ab:\n    heatmap = make_gradcam_heatmap(arr, model, last_conv_layer)\n    score = explanation_complexity(heatmap)\n    scores.append(score)\ndf.at[\"Grad-CAM\", \"Complexity\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________RANDOMIZATION\ndef explanation_randomization_check(original_expl, randomized_expl):\n    \"\"\"\n    Compares similarity between original and randomized explanations.\n    \"\"\"\n    from scipy.stats import spearmanr\n    orig_flat = original_expl.flatten()\n    rand_flat = randomized_expl.flatten()\n    corr, _ = spearmanr(orig_flat, rand_flat)\n    return corr  \n\n\n#RANDOM MODEL\nrandomized_model = tf.keras.models.clone_model(model)\nrandomized_model.build((None, 512, 512, 1))\nfor layer in randomized_model.layers:\n    if hasattr(layer, 'kernel_initializer'):\n        layer.kernel.assign(layer.kernel_initializer(tf.shape(layer.kernel)))\n    if hasattr(layer, 'bias_initializer') and layer.bias is not None:\n        layer.bias.assign(layer.bias_initializer(tf.shape(layer.bias)))\nscores = []\n\nfor arr in ab:\n    heatmap = make_gradcam_heatmap(arr, model, last_conv_layer)\n    random_heatmap = make_gradcam_heatmap(arr, randomized_model, last_conv_layer)\n    score = explanation_randomization_check(heatmap, random_heatmap)\n    scores.append(score)\n\ndf.at[\"Grad-CAM\", \"Randomization\"] = round(np.array(scores).mean(), 3)\ndisplay(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T23:36:32.166646Z","iopub.execute_input":"2025-04-24T23:36:32.167637Z","iopub.status.idle":"2025-04-24T23:37:18.131122Z","shell.execute_reply.started":"2025-04-24T23:36:32.167605Z","shell.execute_reply":"2025-04-24T23:37:18.130257Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                       Robustness Faithfulness Complexity Randomization\nGrad-CAM                    7.878        0.577      0.522        -0.144\nGrad-CAM++                      -            -          -             -\nScore-CAM                       -            -          -             -\nIntegrated Gradients            -            -          -             -\nSmoothGrad                      -            -          -             -\nOcclusion Sensitivity           -            -          -             -\nGradCAM + Lung segment          -            -          -             -","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Robustness</th>\n      <th>Faithfulness</th>\n      <th>Complexity</th>\n      <th>Randomization</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Grad-CAM</th>\n      <td>7.878</td>\n      <td>0.577</td>\n      <td>0.522</td>\n      <td>-0.144</td>\n    </tr>\n    <tr>\n      <th>Grad-CAM++</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>Score-CAM</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>Integrated Gradients</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>SmoothGrad</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>Occlusion Sensitivity</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>GradCAM + Lung segment</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"<center><h1>Grad-CAM ++</h1></center>","metadata":{}},{"cell_type":"code","source":"#________________________________________________________________________________________________________________________GRAD-CAM++\n#____________________________________________________________________________________________ROBUSTNESS\n\n\ndef explanation_robustness(original_explanation, image):\n    epsilon=0.01\n    noise = np.random.normal(loc=0.0, scale=epsilon, size=image.shape)\n    perturbed_img = np.clip(image + noise, 0, 1)\n    expl = make_gradcam_plus_plus_heatmap(perturbed_img, model, last_conv_layer)\n    diffs = np.linalg.norm(original_explanation - expl)\n    return round(np.mean(diffs), 3)\nscores = []\n    \nfor arr in ab:\n    heatmap = make_gradcam_heatmap(arr, model, last_conv_layer)\n    score = explanation_robustness(heatmap, arr)\n    scores.append(score)    \n    \n        \ndf.at[\"Grad-CAM++\", \"Robustness\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________FAITHFULNESS\ndef faithfulness(model, image):        \n    k=0.20\n    p0 = round(model.predict(image, verbose=0)[0][0], 3)\n    heatmap = make_gradcam_plus_plus_heatmap(image, model, last_conv_layer)\n    threshold = np.percentile(heatmap, 100 - k*100)\n    mask = (heatmap >= threshold).astype(bool)\n        \n    image_deleted = np.copy(tf.squeeze(image).numpy())\n    image_deleted = np.expand_dims(image_deleted, axis=-1) \n    image_deleted[mask] = 0 \n    \n    image_deleted = np.expand_dims(image_deleted, 0)\n    p1 = round(model.predict(image_deleted, verbose=0)[0][0], 3)\n    return abs(p0-p1)\n\nscores = []\nfor arr in ab:\n    score = faithfulness(model, arr)\n    scores.append(score)\ndf.at[\"Grad-CAM++\", \"Faithfulness\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________COMPLEXITY\ndef explanation_complexity(explanation, threshold=0.1):\n    \"\"\"\n    Measures the sparsity (number of important pixels).\n    \"\"\"\n    total_pixels = explanation.size\n    important_pixels = np.sum(explanation > threshold)\n    return important_pixels / total_pixels  # lower = more interpretable\n\n\nscores = []\n\nfor arr in ab:\n    heatmap = make_gradcam_plus_plus_heatmap(arr, model, last_conv_layer)\n    score = explanation_complexity(heatmap)\n    scores.append(score)\ndf.at[\"Grad-CAM++\", \"Complexity\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________RANDOMIZATION\ndef explanation_randomization_check(original_expl, randomized_expl):\n    \"\"\"\n    Compares similarity between original and randomized explanations.\n    \"\"\"\n    from scipy.stats import spearmanr\n    orig_flat = original_expl.flatten()\n    rand_flat = randomized_expl.flatten()\n    corr, _ = spearmanr(orig_flat, rand_flat)\n    return corr  # lower = more different, good for sanity check\n\n\n#RANDOM MODEL\nrandomized_model = tf.keras.models.clone_model(model)\nrandomized_model.build((None, 512, 512, 1))\nfor layer in randomized_model.layers:\n    if hasattr(layer, 'kernel_initializer'):\n        layer.kernel.assign(layer.kernel_initializer(tf.shape(layer.kernel)))\n    if hasattr(layer, 'bias_initializer') and layer.bias is not None:\n        layer.bias.assign(layer.bias_initializer(tf.shape(layer.bias)))\nscores = []\n\nfor arr in ab:\n    heatmap = make_gradcam_plus_plus_heatmap(arr, model, last_conv_layer)\n    random_heatmap = make_gradcam_heatmap(arr, randomized_model, last_conv_layer)\n    score = explanation_randomization_check(heatmap, random_heatmap)\n    scores.append(score)\n\ndf.at[\"Grad-CAM++\", \"Randomization\"] = round(np.array(scores).mean(), 3)\ndisplay(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T23:51:32.626125Z","iopub.execute_input":"2025-04-24T23:51:32.626390Z","iopub.status.idle":"2025-04-24T23:52:42.172959Z","shell.execute_reply.started":"2025-04-24T23:51:32.626370Z","shell.execute_reply":"2025-04-24T23:52:42.172276Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                        Robustness Faithfulness Complexity Randomization\nGrad-CAM                     7.878        0.577      0.522        -0.144\nGrad-CAM++              135.330994        0.302      0.534         0.016\nScore-CAM                        -            -          -             -\nIntegrated Gradients             -            -          -             -\nSmoothGrad                       -            -          -             -\nOcclusion Sensitivity            -            -          -             -\nGradCAM + Lung segment           -            -          -             -","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Robustness</th>\n      <th>Faithfulness</th>\n      <th>Complexity</th>\n      <th>Randomization</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Grad-CAM</th>\n      <td>7.878</td>\n      <td>0.577</td>\n      <td>0.522</td>\n      <td>-0.144</td>\n    </tr>\n    <tr>\n      <th>Grad-CAM++</th>\n      <td>135.330994</td>\n      <td>0.302</td>\n      <td>0.534</td>\n      <td>0.016</td>\n    </tr>\n    <tr>\n      <th>Score-CAM</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>Integrated Gradients</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>SmoothGrad</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>Occlusion Sensitivity</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>GradCAM + Lung segment</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"<center><h1>SCORE CAM </h1></center>","metadata":{}},{"cell_type":"code","source":"#________________________________________________________________________________________________________________________SCORE CAM\n#____________________________________________________________________________________________ROBUSTNESS\n\nnew_list = images_arr\n\noriginal_scorecams = [scorecam_heatmap(arr, model, last_conv_layer) for arr in new_list]\n\ndef explanation_robustness(original_explanation, image):\n    epsilon = 0.01\n    noise = np.random.normal(loc=0.0, scale=epsilon, size=image.shape)\n    perturbed_img = np.clip(image + noise, 0, 1)\n    expl = scorecam_heatmap(perturbed_img, model, last_conv_layer)\n    diffs = np.linalg.norm(original_explanation - expl)\n    return round(np.mean(diffs), 3)\n\nscores = []\nfor arr, heatmap in zip(new_list, original_scorecams):\n    score = explanation_robustness(heatmap, arr)\n    scores.append(score)\n    \ndf.at[\"Score-CAM\", \"Robustness\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________FAITHFULNESS\ndef faithfulness(model, image, heatmap=None):        \n    k = 0.20\n    p0 = round(model.predict(image, verbose=0)[0][0], 3)\n    \n    if heatmap is None:\n        heatmap = scorecam_heatmap(image, model, last_conv_layer)\n        \n    threshold = np.percentile(heatmap, 100 - k*100)\n    mask = (heatmap >= threshold).astype(bool)\n        \n    image_deleted = np.copy(tf.squeeze(image).numpy())\n    image_deleted = np.expand_dims(image_deleted, axis=-1) \n    image_deleted[mask] = 0 \n    \n    image_deleted = np.expand_dims(image_deleted, 0)\n    p1 = round(model.predict(image_deleted, verbose=0)[0][0], 3)\n    return abs(p0 - p1)\n\nscores = []\nfor arr, heatmap in zip(new_list, original_scorecams):\n    score = faithfulness(model, arr, heatmap)\n    scores.append(score)\n    \ndf.at[\"Score-CAM\", \"Faithfulness\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________COMPLEXITY\ndef explanation_complexity(explanation, threshold=0.1):\n    \"\"\"Measures the sparsity (number of important pixels).\"\"\"\n    total_pixels = explanation.size\n    important_pixels = np.sum(explanation > threshold)\n    return important_pixels / total_pixels  # lower = more interpretable\n\nscores = []\nfor heatmap in original_scorecams:\n    score = explanation_complexity(heatmap)\n    scores.append(score)\n    \ndf.at[\"Score-CAM\", \"Complexity\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________RANDOMIZATION\ndef explanation_randomization_check(original_expl, randomized_expl):\n    \"\"\"Compares similarity between original and randomized explanations.\"\"\"\n    from scipy.stats import spearmanr\n    orig_flat = original_expl.flatten()\n    rand_flat = randomized_expl.flatten()\n    corr, _ = spearmanr(orig_flat, rand_flat)\n    return corr  # lower = more different, good for sanity check\n\n# Create randomized model (only once)\nrandomized_model = tf.keras.models.clone_model(model)\nrandomized_model.build((None, 512, 512, 1))\nfor layer in randomized_model.layers:\n    if hasattr(layer, 'kernel_initializer'):\n        layer.kernel.assign(layer.kernel_initializer(tf.shape(layer.kernel)))\n    if hasattr(layer, 'bias_initializer') and layer.bias is not None:\n        layer.bias.assign(layer.bias_initializer(tf.shape(layer.bias)))\n\n# Precompute all random Grad-CAM heatmaps once\nrandom_gradcams = [make_gradcam_heatmap(arr, randomized_model, last_conv_layer) for arr in new_list]\n\nscores = []\nfor orig_heatmap, rand_heatmap in zip(original_scorecams, random_gradcams):\n    score = explanation_randomization_check(orig_heatmap, rand_heatmap)\n    scores.append(score)\n\ndf.at[\"Score-CAM\", \"Randomization\"] = round(np.array(scores).mean(), 3)\n\ndisplay(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T23:56:39.043447Z","iopub.execute_input":"2025-04-24T23:56:39.044006Z","iopub.status.idle":"2025-04-25T00:06:15.348229Z","shell.execute_reply.started":"2025-04-24T23:56:39.043982Z","shell.execute_reply":"2025-04-25T00:06:15.347439Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                        Robustness Faithfulness Complexity Randomization\nGrad-CAM                     7.878        0.577      0.522        -0.144\nGrad-CAM++              135.330994        0.302      0.534         0.016\nScore-CAM                    1.628        0.127      0.994         0.069\nIntegrated Gradients             -            -          -             -\nSmoothGrad                       -            -          -             -\nOcclusion Sensitivity            -            -          -             -\nGradCAM + Lung segment           -            -          -             -","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Robustness</th>\n      <th>Faithfulness</th>\n      <th>Complexity</th>\n      <th>Randomization</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Grad-CAM</th>\n      <td>7.878</td>\n      <td>0.577</td>\n      <td>0.522</td>\n      <td>-0.144</td>\n    </tr>\n    <tr>\n      <th>Grad-CAM++</th>\n      <td>135.330994</td>\n      <td>0.302</td>\n      <td>0.534</td>\n      <td>0.016</td>\n    </tr>\n    <tr>\n      <th>Score-CAM</th>\n      <td>1.628</td>\n      <td>0.127</td>\n      <td>0.994</td>\n      <td>0.069</td>\n    </tr>\n    <tr>\n      <th>Integrated Gradients</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>SmoothGrad</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>Occlusion Sensitivity</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>GradCAM + Lung segment</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"<center><h1>OCCLUSION</h1></center>","metadata":{}},{"cell_type":"code","source":"#________________________________________________________________________________________________________________________OCCLUSION\n#____________________________________________________________________________________________ROBUSTNESS\n\nlistt = images_arr\noriginal_heatmaps = [compute_occlusion_sensitivity(arr, model) for arr in listt]\n\ndef explanation_robustness(original_explanation, image):\n    epsilon = 0.01\n    noise = np.random.normal(loc=0.0, scale=epsilon, size=image.shape)\n    perturbed_img = np.clip(image + noise, 0, 1)\n    expl = compute_occlusion_sensitivity(perturbed_img, model)\n    diffs = np.linalg.norm(original_explanation - expl)\n    return round(np.mean(diffs), 3)\n\nscores = []\nfor arr, heatmap in zip(listt, original_heatmaps):\n    score = explanation_robustness(heatmap, arr)\n    scores.append(score)\n    \ndf.at[\"Occlusion Sensitivity\", \"Robustness\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________FAITHFULNESS\ndef faithfulness(model, image, heatmap=None):        \n    k = 0.20\n    p0 = round(model.predict(image, verbose=0)[0][0], 3)\n    \n    if heatmap is None:\n        heatmap = compute_occlusion_sensitivity(image, model)\n        \n    threshold = np.percentile(heatmap, 100 - k*100)\n    mask = (heatmap >= threshold).astype(bool)\n        \n    image_deleted = np.copy(tf.squeeze(image).numpy())\n    image_deleted = np.expand_dims(image_deleted, axis=-1) \n    image_deleted[mask] = 0 \n    \n    image_deleted = np.expand_dims(image_deleted, 0)\n    p1 = round(model.predict(image_deleted, verbose=0)[0][0], 3)\n    return abs(p0 - p1)\n\nscores = []\nfor arr, heatmap in zip(listt, original_heatmaps):\n    score = faithfulness(model, arr, heatmap)\n    scores.append(score)\n    \ndf.at[\"Occlusion Sensitivity\", \"Faithfulness\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________COMPLEXITY\ndef explanation_complexity(explanation, threshold=0.1):\n    \"\"\"Measures the sparsity (number of important pixels).\"\"\"\n    total_pixels = explanation.size\n    important_pixels = np.sum(explanation > threshold)\n    return important_pixels / total_pixels  # lower = more interpretable\n\nscores = []\nfor heatmap in original_heatmaps:\n    score = explanation_complexity(heatmap)\n    scores.append(score)\n    \ndf.at[\"Occlusion Sensitivity\", \"Complexity\"] = round(np.array(scores).mean(), 3)\n\n\n\n#____________________________________________________________________________________________RANDOMIZATION\ndef explanation_randomization_check(original_expl, randomized_expl):\n    \"\"\"Compares similarity between original and randomized explanations.\"\"\"\n    from scipy.stats import spearmanr\n    orig_flat = original_expl.flatten()\n    rand_flat = randomized_expl.flatten()\n    corr, _ = spearmanr(orig_flat, rand_flat)\n    return corr  # lower = more different, good for sanity check\n\n# Create randomized model\nrandomized_model = tf.keras.models.clone_model(model)\nrandomized_model.build((None, 512, 512, 1))\nfor layer in randomized_model.layers:\n    if hasattr(layer, 'kernel_initializer'):\n        layer.kernel.assign(layer.kernel_initializer(tf.shape(layer.kernel)))\n    if hasattr(layer, 'bias_initializer') and layer.bias is not None:\n        layer.bias.assign(layer.bias_initializer(tf.shape(layer.bias)))\n\n# Precompute random heatmaps once\nrandom_heatmaps = [compute_occlusion_sensitivity(arr, randomized_model) for arr in listt]\n\nscores = []\nfor orig_heatmap, rand_heatmap in zip(original_heatmaps, random_heatmaps):\n    score = explanation_randomization_check(orig_heatmap, rand_heatmap)\n    scores.append(score)\n\ndf.at[\"Occlusion Sensitivity\", \"Randomization\"] = round(np.array(scores).mean(), 3)\n\ndisplay(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T01:04:37.636322Z","iopub.execute_input":"2025-04-25T01:04:37.637077Z","iopub.status.idle":"2025-04-25T01:11:03.858144Z","shell.execute_reply.started":"2025-04-25T01:04:37.637049Z","shell.execute_reply":"2025-04-25T01:11:03.857475Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                        Robustness Faithfulness Complexity Randomization\nGrad-CAM                     7.878        0.577      0.522        -0.144\nGrad-CAM++              135.330994        0.302      0.534         0.016\nScore-CAM                    1.628        0.127      0.994         0.069\nIntegrated Gradients             -            -          -             -\nSmoothGrad                   6.446        0.197      0.012         0.006\nOcclusion Sensitivity        5.526        0.184      0.287        -0.022\nGradCAM + Lung segment           -            -          -             -","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Robustness</th>\n      <th>Faithfulness</th>\n      <th>Complexity</th>\n      <th>Randomization</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Grad-CAM</th>\n      <td>7.878</td>\n      <td>0.577</td>\n      <td>0.522</td>\n      <td>-0.144</td>\n    </tr>\n    <tr>\n      <th>Grad-CAM++</th>\n      <td>135.330994</td>\n      <td>0.302</td>\n      <td>0.534</td>\n      <td>0.016</td>\n    </tr>\n    <tr>\n      <th>Score-CAM</th>\n      <td>1.628</td>\n      <td>0.127</td>\n      <td>0.994</td>\n      <td>0.069</td>\n    </tr>\n    <tr>\n      <th>Integrated Gradients</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>SmoothGrad</th>\n      <td>6.446</td>\n      <td>0.197</td>\n      <td>0.012</td>\n      <td>0.006</td>\n    </tr>\n    <tr>\n      <th>Occlusion Sensitivity</th>\n      <td>5.526</td>\n      <td>0.184</td>\n      <td>0.287</td>\n      <td>-0.022</td>\n    </tr>\n    <tr>\n      <th>GradCAM + Lung segment</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"<center><h1>SMOOTHGRAD</h1></center>","metadata":{}},{"cell_type":"code","source":"#________________________________________________________________________________________________________________________SMOOTHGRAD\n#____________________________________________________________________________________________ROBUSTNESS\nlistt = []\nlistt = images_arr\n\n# Precompute all heatmaps once\noriginal_heatmaps = [make_smoothgrad_heatmap(arr, model) for arr in listt]\n\ndef explanation_robustness(original_explanation, image):\n    epsilon = 0.01\n    noise = np.random.normal(loc=0.0, scale=epsilon, size=image.shape)\n    perturbed_img = np.clip(image + noise, 0, 1)\n    expl = make_smoothgrad_heatmap(perturbed_img, model)\n    diffs = np.linalg.norm(original_explanation - expl)\n    return round(np.mean(diffs), 3)\n\nscores = []\nfor arr, heatmap in zip(listt, original_heatmaps):\n    score = explanation_robustness(heatmap, arr)\n    scores.append(score)\n    \ndf.at[\"SmoothGrad\", \"Robustness\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________FAITHFULNESS\ndef faithfulness(model, image, heatmap=None):        \n    k = 0.20\n    p0 = round(model.predict(image, verbose=0)[0][0], 3)\n    \n    if heatmap is None:\n        heatmap = make_smoothgrad_heatmap(image, model)\n        \n    threshold = np.percentile(heatmap, 100 - k*100)\n    mask = (heatmap >= threshold).astype(bool)\n        \n    image_deleted = np.copy(tf.squeeze(image).numpy())\n    image_deleted = np.expand_dims(image_deleted, axis=-1) \n    image_deleted[mask] = 0 \n    \n    image_deleted = np.expand_dims(image_deleted, 0)\n    p1 = round(model.predict(image_deleted, verbose=0)[0][0], 3)\n    return abs(p0 - p1)\n\nscores = []\nfor arr, heatmap in zip(listt, original_heatmaps):\n    score = faithfulness(model, arr, heatmap)\n    scores.append(score)\n    \ndf.at[\"SmoothGrad\", \"Faithfulness\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________COMPLEXITY\ndef explanation_complexity(explanation, threshold=0.1):\n    \"\"\"Measures the sparsity (number of important pixels).\"\"\"\n    total_pixels = explanation.size\n    important_pixels = np.sum(explanation > threshold)\n    return important_pixels / total_pixels  # lower = more interpretable\n\nscores = []\nfor heatmap in original_heatmaps:\n    score = explanation_complexity(heatmap)\n    scores.append(score)\n    \ndf.at[\"SmoothGrad\", \"Complexity\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________RANDOMIZATION\ndef explanation_randomization_check(original_expl, randomized_expl):\n    \"\"\"Compares similarity between original and randomized explanations.\"\"\"\n    from scipy.stats import spearmanr\n    orig_flat = original_expl.flatten()\n    rand_flat = randomized_expl.flatten()\n    corr, _ = spearmanr(orig_flat, rand_flat)\n    return corr  # lower = more different, good for sanity check\n\n# Create randomized model\nrandomized_model = tf.keras.models.clone_model(model)\nrandomized_model.build((None, 512, 512, 1))\nfor layer in randomized_model.layers:\n    if hasattr(layer, 'kernel_initializer'):\n        layer.kernel.assign(layer.kernel_initializer(tf.shape(layer.kernel)))\n    if hasattr(layer, 'bias_initializer') and layer.bias is not None:\n        layer.bias.assign(layer.bias_initializer(tf.shape(layer.bias)))\n\n# Precompute random heatmaps once\nrandom_heatmaps = [make_smoothgrad_heatmap(arr, randomized_model) for arr in listt]\n\nscores = []\nfor orig_heatmap, rand_heatmap in zip(original_heatmaps, random_heatmaps):\n    score = explanation_randomization_check(orig_heatmap, rand_heatmap)\n    scores.append(score)\n# Calculate mean of non-nan values\nvalid_scores = [x for x in scores if not np.isnan(x)]\nmean_score = np.mean(valid_scores)\n\nscores = [mean_score if np.isnan(x) else x for x in scores]\ndf.at[\"SmoothGrad\", \"Randomization\"] = round(np.array(scores).mean(), 3)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T00:54:51.878358Z","iopub.execute_input":"2025-04-25T00:54:51.878991Z","iopub.status.idle":"2025-04-25T01:00:14.691527Z","shell.execute_reply.started":"2025-04-25T00:54:51.878965Z","shell.execute_reply":"2025-04-25T01:00:14.690791Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/2244567500.py:70: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n  corr, _ = spearmanr(orig_flat, rand_flat)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                        Robustness Faithfulness Complexity Randomization\nGrad-CAM                     7.878        0.577      0.522        -0.144\nGrad-CAM++              135.330994        0.302      0.534         0.016\nScore-CAM                    1.628        0.127      0.994         0.069\nIntegrated Gradients             -            -          -             -\nSmoothGrad                   6.446        0.197      0.012         0.006\nOcclusion Sensitivity        5.526        0.184      0.287             -\nGradCAM + Lung segment           -            -          -             -","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Robustness</th>\n      <th>Faithfulness</th>\n      <th>Complexity</th>\n      <th>Randomization</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Grad-CAM</th>\n      <td>7.878</td>\n      <td>0.577</td>\n      <td>0.522</td>\n      <td>-0.144</td>\n    </tr>\n    <tr>\n      <th>Grad-CAM++</th>\n      <td>135.330994</td>\n      <td>0.302</td>\n      <td>0.534</td>\n      <td>0.016</td>\n    </tr>\n    <tr>\n      <th>Score-CAM</th>\n      <td>1.628</td>\n      <td>0.127</td>\n      <td>0.994</td>\n      <td>0.069</td>\n    </tr>\n    <tr>\n      <th>Integrated Gradients</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>SmoothGrad</th>\n      <td>6.446</td>\n      <td>0.197</td>\n      <td>0.012</td>\n      <td>0.006</td>\n    </tr>\n    <tr>\n      <th>Occlusion Sensitivity</th>\n      <td>5.526</td>\n      <td>0.184</td>\n      <td>0.287</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>GradCAM + Lung segment</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"display(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T01:11:49.057799Z","iopub.execute_input":"2025-04-25T01:11:49.058081Z","iopub.status.idle":"2025-04-25T01:11:49.066133Z","shell.execute_reply.started":"2025-04-25T01:11:49.058064Z","shell.execute_reply":"2025-04-25T01:11:49.065447Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                        Robustness Faithfulness Complexity Randomization\nGrad-CAM                     7.878        0.577      0.522        -0.144\nGrad-CAM++              135.330994        0.302      0.534         0.016\nScore-CAM                    1.628        0.127      0.994         0.069\nIntegrated Gradients             -            -          -             -\nSmoothGrad                   6.446        0.197      0.012         0.006\nOcclusion Sensitivity        5.526        0.184      0.287        -0.022\nGradCAM + Lung segment           -            -          -             -","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Robustness</th>\n      <th>Faithfulness</th>\n      <th>Complexity</th>\n      <th>Randomization</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Grad-CAM</th>\n      <td>7.878</td>\n      <td>0.577</td>\n      <td>0.522</td>\n      <td>-0.144</td>\n    </tr>\n    <tr>\n      <th>Grad-CAM++</th>\n      <td>135.330994</td>\n      <td>0.302</td>\n      <td>0.534</td>\n      <td>0.016</td>\n    </tr>\n    <tr>\n      <th>Score-CAM</th>\n      <td>1.628</td>\n      <td>0.127</td>\n      <td>0.994</td>\n      <td>0.069</td>\n    </tr>\n    <tr>\n      <th>Integrated Gradients</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>SmoothGrad</th>\n      <td>6.446</td>\n      <td>0.197</td>\n      <td>0.012</td>\n      <td>0.006</td>\n    </tr>\n    <tr>\n      <th>Occlusion Sensitivity</th>\n      <td>5.526</td>\n      <td>0.184</td>\n      <td>0.287</td>\n      <td>-0.022</td>\n    </tr>\n    <tr>\n      <th>GradCAM + Lung segment</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"<center><h1>INTEGRATED GRADIENTS</h1></center>","metadata":{}},{"cell_type":"code","source":"#________________________________________________________________________________________________________________________INTEGRATED GRADIENTS\n#____________________________________________________________________________________________ROBUSTNESS\nlistt = []\nlistt = images_arr\n\n# Precompute all heatmaps once\noriginal_heatmaps = [make_integrated_gradients_heatmap(arr, model) for arr in listt]\ndef explanation_robustness(original_explanation, image):\n    epsilon = 0.01\n    noise = np.random.normal(loc=0.0, scale=epsilon, size=image.shape)\n    perturbed_img = np.clip(image + noise, 0, 1)\n    expl = make_integrated_gradients_heatmap(perturbed_img, model)\n    diffs = np.linalg.norm(original_explanation - expl)\n    return round(np.mean(diffs), 3)\n\nscores = []\nfor arr, heatmap in zip(listt, original_heatmaps):\n    score = explanation_robustness(heatmap, arr)\n    scores.append(score)\n    \ndf.at[\"Integrated Gradients\", \"Robustness\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________FAITHFULNESS\ndef faithfulness(model, image, heatmap=None):        \n    k = 0.20\n    p0 = round(model.predict(image, verbose=0)[0][0], 3)\n    \n    if heatmap is None:\n        heatmap = make_integrated_gradients_heatmap(image, model)\n        \n    threshold = np.percentile(heatmap, 100 - k*100)\n    mask = (heatmap >= threshold).astype(bool)\n        \n    image_deleted = np.copy(tf.squeeze(image).numpy())\n    image_deleted = np.expand_dims(image_deleted, axis=-1) \n    image_deleted[mask] = 0 \n    \n    image_deleted = np.expand_dims(image_deleted, 0)\n    p1 = round(model.predict(image_deleted, verbose=0)[0][0], 3)\n    return abs(p0 - p1)\n\nscores = []\nfor arr, heatmap in zip(listt, original_heatmaps):\n    score = faithfulness(model, arr, heatmap)\n    scores.append(score)\n    \ndf.at[\"Integrated Gradients\", \"Faithfulness\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________COMPLEXITY\ndef explanation_complexity(explanation, threshold=0.1):\n    \"\"\"Measures the sparsity (number of important pixels).\"\"\"\n    total_pixels = explanation.size\n    important_pixels = np.sum(explanation > threshold)\n    return important_pixels / total_pixels  # lower = more interpretable\n\nscores = []\nfor heatmap in original_heatmaps:\n    score = explanation_complexity(heatmap)\n    scores.append(score)\n    \ndf.at[\"Integrated Gradients\", \"Complexity\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________RANDOMIZATION\ndef explanation_randomization_check(original_expl, randomized_expl):\n    \"\"\"Compares similarity between original and randomized explanations.\"\"\"\n    from scipy.stats import spearmanr\n    orig_flat = original_expl.flatten()\n    rand_flat = randomized_expl.flatten()\n    corr, _ = spearmanr(orig_flat, rand_flat)\n    return corr  # lower = more different, good for sanity check\n\n# Create randomized model\nrandomized_model = tf.keras.models.clone_model(model)\nrandomized_model.build((None, 512, 512, 1))\nfor layer in randomized_model.layers:\n    if hasattr(layer, 'kernel_initializer'):\n        layer.kernel.assign(layer.kernel_initializer(tf.shape(layer.kernel)))\n    if hasattr(layer, 'bias_initializer') and layer.bias is not None:\n        layer.bias.assign(layer.bias_initializer(tf.shape(layer.bias)))\n\n# Precompute random heatmaps once\nrandom_heatmaps = [make_integrated_gradients_heatmap(arr, randomized_model) for arr in listt]\n\nscores = []\nfor orig_heatmap, rand_heatmap in zip(original_heatmaps, random_heatmaps):\n    score = explanation_randomization_check(orig_heatmap, rand_heatmap)\n    scores.append(score)\n\ndf.at[\"Integrated Gradients\", \"Randomization\"] = round(np.array(scores).mean(), 3)\n\ndisplay(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T01:12:54.026630Z","iopub.execute_input":"2025-04-25T01:12:54.027186Z","iopub.status.idle":"2025-04-25T01:13:14.938525Z","shell.execute_reply.started":"2025-04-25T01:12:54.027163Z","shell.execute_reply":"2025-04-25T01:13:14.937815Z"}},"outputs":[{"name":"stdout","text":"hello\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                        Robustness Faithfulness Complexity Randomization\nGrad-CAM                     7.878        0.577      0.522        -0.144\nGrad-CAM++              135.330994        0.302      0.534         0.016\nScore-CAM                    1.628        0.127      0.994         0.069\nIntegrated Gradients         21.68         0.25        1.0           0.0\nSmoothGrad                   6.446        0.197      0.012         0.006\nOcclusion Sensitivity        5.526        0.184      0.287        -0.022\nGradCAM + Lung segment           -            -          -             -","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Robustness</th>\n      <th>Faithfulness</th>\n      <th>Complexity</th>\n      <th>Randomization</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Grad-CAM</th>\n      <td>7.878</td>\n      <td>0.577</td>\n      <td>0.522</td>\n      <td>-0.144</td>\n    </tr>\n    <tr>\n      <th>Grad-CAM++</th>\n      <td>135.330994</td>\n      <td>0.302</td>\n      <td>0.534</td>\n      <td>0.016</td>\n    </tr>\n    <tr>\n      <th>Score-CAM</th>\n      <td>1.628</td>\n      <td>0.127</td>\n      <td>0.994</td>\n      <td>0.069</td>\n    </tr>\n    <tr>\n      <th>Integrated Gradients</th>\n      <td>21.68</td>\n      <td>0.25</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>SmoothGrad</th>\n      <td>6.446</td>\n      <td>0.197</td>\n      <td>0.012</td>\n      <td>0.006</td>\n    </tr>\n    <tr>\n      <th>Occlusion Sensitivity</th>\n      <td>5.526</td>\n      <td>0.184</td>\n      <td>0.287</td>\n      <td>-0.022</td>\n    </tr>\n    <tr>\n      <th>GradCAM + Lung segment</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"<center><h1>GRAD-CAM  +  LUNG SEGMENTATION</h1></center>","metadata":{}},{"cell_type":"code","source":"\ndef mask_lung(img_path, s_model=segmentation_model):\n    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    #Segmentation model\n    original = cv2.resize(img, (256, 256))\n    original = original.astype(\"float32\") / 255.0\n    original = np.expand_dims(original, axis=-1) \n    original = np.expand_dims(original, axis=0)\n    git_seg = s_model.predict(original, verbose=0)\n    git_seg = np.squeeze(git_seg)\n\n    #openCV\n    equalized = cv2.equalizeHist(img)\n\n    blurred = cv2.GaussianBlur(equalized, (5, 5), 0)\n    first = cv2.resize(blurred, (256, 256))\n    first = first.astype(\"float32\") / 255.0\n    first = np.expand_dims(first, axis=-1) \n    first = np.expand_dims(first, axis=0)\n    first_seg = s_model.predict(first, verbose=0)\n    first_seg = np.squeeze(first_seg) \n\n    _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    kernel = np.ones((5, 5), np.uint8)\n\n    opened = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n    second = cv2.resize(opened, (256, 256))\n    second = second.astype(\"float32\") / 255.0\n    second = np.expand_dims(second, axis=-1) \n    second = np.expand_dims(second, axis=0)\n    second_seg = s_model.predict(second, verbose=0)\n    second_seg = np.squeeze(second_seg)\n\n    contours, _ = cv2.findContours(opened, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    lung_mask = np.zeros_like(img)\n    cv2.drawContours(lung_mask, contours, -1, 255, thickness=cv2.FILLED)\n    \n    segmented = cv2.bitwise_and(img, img, mask=lung_mask)\n    third = cv2.resize(segmented, (256, 256))\n    third = third.astype(\"float32\") / 255.0\n    third = np.expand_dims(third, axis=-1) \n    third = np.expand_dims(third, axis=0)\n    third_seg = s_model.predict(third, verbose=0)\n    third_seg = np.squeeze(third_seg)\n\n    mask = np.maximum.reduce([git_seg, first_seg, second_seg, third_seg])\n    mask = np.expand_dims(mask, axis=-1) \n    mask = tf.image.resize(mask, [512, 512], method='bilinear')\n    return mask\n    \ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    grad_model = keras.models.Model(\n        model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    \n    heatmap = tf.squeeze(heatmap)\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    heatmap = tf.image.resize(\n        heatmap[..., tf.newaxis],  \n        (512, 512),\n        method=tf.image.ResizeMethod.BILINEAR  \n    )\n    return heatmap.numpy()\n\n\ndef meaningful_expl_heatmap(img_array, img_path, model):\n    mask = mask_lung(img_path)\n    img_array = get_arr_img(img_path)\n    grad_cam = make_gradcam_heatmap(img_array, model, last_conv_layer)\n    masked_heatmap = grad_cam * mask\n    return masked_heatmap.numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T10:33:40.591602Z","iopub.execute_input":"2025-04-25T10:33:40.592614Z","iopub.status.idle":"2025-04-25T10:33:40.609545Z","shell.execute_reply.started":"2025-04-25T10:33:40.592574Z","shell.execute_reply":"2025-04-25T10:33:40.608646Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#________________________________________________________________________________________________________________________GRAD-CAM+ LUNG SEGMENTATION\n#____________________________________________________________________________________________ROBUSTNESS\n\nab = []\nwith open(\"/kaggle/input/random-paths/random_paths.txt\", \"r\") as file:\n    for line in file:\n        path = line.strip()\n        ab.append(path)\n\ndef explanation_robustness(original_explanation, image, path, t_model):\n    epsilon=0.01\n    noise = np.random.normal(loc=0.0, scale=epsilon, size=image.shape)\n    perturbed_img = np.clip(image + noise, 0, 1)\n    expl = meaningful_expl_heatmap(perturbed_img, path, t_model)\n    diffs = np.linalg.norm(original_explanation - expl)\n    return round(np.mean(diffs), 3)\nscores = []\n    \nfor p in ab:\n    arr = get_arr_img(p)\n    heatmap = meaningful_expl_heatmap(arr, p, model)\n    score = explanation_robustness(heatmap, arr, p, model)\n    scores.append(score)    \n    \n        \ndf.at[\"GradCAM + Lung segment\", \"Robustness\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________FAITHFULNESS\ndef faithfulness(model, image, path, t_model):        \n    k=0.20\n    p0 = round(model.predict(image, verbose=0)[0][0], 3)\n    heatmap = meaningful_expl_heatmap(image, path, t_model)\n    threshold = np.percentile(heatmap, 100 - k*100)\n    #heatmap = heatmap.numpy()\n    mask = (heatmap >= threshold).astype(bool)\n        \n    image_deleted = np.copy(tf.squeeze(image).numpy())\n    image_deleted = np.expand_dims(image_deleted, axis=-1) \n    image_deleted[mask] = 0 \n    \n    image_deleted = np.expand_dims(image_deleted, 0)\n    p1 = round(model.predict(image_deleted, verbose=0)[0][0], 3)\n    return abs(p0-p1)\n\nscores = []\n\nfor p in ab:\n    arr = get_arr_img(p)\n    score = faithfulness(model, arr, p, model)\n    scores.append(score)\ndf.at[\"GradCAM + Lung segment\", \"Faithfulness\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________COMPLEXITY\ndef explanation_complexity(explanation, threshold=0.1):\n    #explanation = explanation.numpy() \n    total_pixels = explanation.size\n    important_pixels = np.sum(explanation > threshold)\n    return important_pixels / total_pixels  # lower = more interpretable\n\n\nscores = []\n\nfor p in ab:\n    arr = get_arr_img(p)\n    heatmap = meaningful_expl_heatmap(arr, p, model)\n    score = explanation_complexity(heatmap)\n    scores.append(score)\ndf.at[\"GradCAM + Lung segment\", \"Complexity\"] = round(np.array(scores).mean(), 3)\n\n#____________________________________________________________________________________________RANDOMIZATION\ndef explanation_randomization_check(original_expl, randomized_expl):\n    \"\"\"\n    Compares similarity between original and randomized explanations.\n    \"\"\"\n    from scipy.stats import spearmanr\n    if hasattr(original_expl, 'numpy'):\n        original_expl = original_expl.numpy()\n        randomized_expl = randomized_expl.numpy()\n    orig_flat = original_expl.flatten()\n    rand_flat = randomized_expl.flatten()\n    corr, _ = spearmanr(orig_flat, rand_flat)\n    return corr  # lower = more different, good for sanity check\n\n\n#RANDOM MODEL\nrandomized_model = tf.keras.models.clone_model(model)\nrandomized_model.build((None, 512, 512, 1))\nfor layer in randomized_model.layers:\n    if hasattr(layer, 'kernel_initializer'):\n        layer.kernel.assign(layer.kernel_initializer(tf.shape(layer.kernel)))\n    if hasattr(layer, 'bias_initializer') and layer.bias is not None:\n        layer.bias.assign(layer.bias_initializer(tf.shape(layer.bias)))\nscores = []\n\nfor p in ab:\n    arr = get_arr_img(p)\n    heatmap = meaningful_expl_heatmap(arr, p, model)\n    random_heatmap = meaningful_expl_heatmap(arr, p, randomized_model)\n    score = explanation_randomization_check(heatmap, random_heatmap)\n    scores.append(score)\n\ndf.at[\"GradCAM + Lung segment\", \"Randomization\"] = round(np.array(scores).mean(), 3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T11:01:56.055723Z","iopub.execute_input":"2025-04-25T11:01:56.056281Z","iopub.status.idle":"2025-04-25T11:01:56.064893Z","shell.execute_reply.started":"2025-04-25T11:01:56.056256Z","shell.execute_reply":"2025-04-25T11:01:56.064086Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                        Robustness  Faithfulness  Complexity  Randomization\nGrad-CAM                  7.878000         0.577       0.522         -0.144\nGrad-CAM++              135.330994         0.302       0.534          0.016\nScore-CAM                 1.628000         0.127       0.994          0.069\nIntegrated Gradients     21.680000         0.250       1.000          0.000\nSmoothGrad                6.446000         0.197       0.012          0.006\nOcclusion Sensitivity     5.526000         0.184       0.287         -0.022\nGradCAM + Lung segment    0.330000         0.737       0.049         -0.208","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Robustness</th>\n      <th>Faithfulness</th>\n      <th>Complexity</th>\n      <th>Randomization</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Grad-CAM</th>\n      <td>7.878000</td>\n      <td>0.577</td>\n      <td>0.522</td>\n      <td>-0.144</td>\n    </tr>\n    <tr>\n      <th>Grad-CAM++</th>\n      <td>135.330994</td>\n      <td>0.302</td>\n      <td>0.534</td>\n      <td>0.016</td>\n    </tr>\n    <tr>\n      <th>Score-CAM</th>\n      <td>1.628000</td>\n      <td>0.127</td>\n      <td>0.994</td>\n      <td>0.069</td>\n    </tr>\n    <tr>\n      <th>Integrated Gradients</th>\n      <td>21.680000</td>\n      <td>0.250</td>\n      <td>1.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>SmoothGrad</th>\n      <td>6.446000</td>\n      <td>0.197</td>\n      <td>0.012</td>\n      <td>0.006</td>\n    </tr>\n    <tr>\n      <th>Occlusion Sensitivity</th>\n      <td>5.526000</td>\n      <td>0.184</td>\n      <td>0.287</td>\n      <td>-0.022</td>\n    </tr>\n    <tr>\n      <th>GradCAM + Lung segment</th>\n      <td>0.330000</td>\n      <td>0.737</td>\n      <td>0.049</td>\n      <td>-0.208</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}